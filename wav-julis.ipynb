{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e28b7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##  Wav2Lip on CPU - Google Colab (Free Tier)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook allows you to run Wav2Lip inference on a CPU, making it compatible with Google Colab's free tier. It includes text-to-speech using gTTS to generate audio from text, which is then used to animate a static face image.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 1. Setup Environment and Install Dependencies\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell will:\\n\",\n",
    "    \"- Clone the Wav2Lip repository.\\n\",\n",
    "    \"- Install specific versions of PyTorch (CPU-compatible) and Librosa (for compatibility).\\n\",\n",
    "    \"- Install other necessary Python packages (gTTS for text-to-speech, opencv-python for image/video processing, etc.).\\n\",\n",
    "    \"- Download the pre-trained Wav2Lip model checkpoint.\\n\",\n",
    "    \"- Download a sample avatar image.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!git clone https://github.com/Rudrabha/Wav2Lip.git\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Install CPU-compatible PyTorch and other dependencies\\n\",\n",
    "    \"!pip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\\n\",\n",
    "    \"!pip install librosa==0.9.2 numba==0.58.1\\n\",\n",
    "    \"!pip install gTTS==2.3.2 opencv-python==4.8.0.76 scipy==1.11.4 tqdm==4.66.1\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download Wav2Lip pre-trained model\\n\",\n",
    "    \"!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EAbENTSj11FFp0Q55_iAIVMBcQx28VpVmTuF4h7RnO00rQ' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download a sample avatar image\\n\",\n",
    "    \"!wget 'https://img.freepik.com/free-photo/young-bearded-man-with-striped-shirt_273609-5677.jpg' -O '/content/avatar.jpg'\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Setup Complete! Wav2Lip repository cloned, dependencies installed, model and sample image downloaded.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2. Import Libraries\\n\",\n",
    "    \"\\n\",\n",
    "    \"Import all the Python libraries that will be used throughout the notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"from gtts import gTTS\\n\",\n",
    "    \"from IPython.display import Audio, HTML, clear_output\\n\",\n",
    "    \"from base64 import b64encode\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import subprocess\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import librosa\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Clear output after imports for a cleaner notebook\\n\",\n",
    "    \"# clear_output()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries imported successfully.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 3. Generate Speech from Text using gTTS\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell defines a function to convert your input text into an audio file (`.wav` format) using Google Text-to-Speech (gTTS). \\n\",\n",
    "    \"The audio will be saved to `/content/generated_tts.wav`.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def text_to_speech(text, output_filename=\\\"/content/generated_tts.wav\\\"):\\n\",\n",
    "    \"    tts = gTTS(text=text, lang='en')\\n\",\n",
    "    \"    tts.save(output_filename)\\n\",\n",
    "    \"    print(f\\\"Text converted to speech and saved as {output_filename}\\\")\\n\",\n",
    "    \"    return output_filename\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Test the TTS --- \\n\",\n",
    "    \"input_text = \\\"Hello, this is a test of the text to speech system for Wav2Lip.\\\" # You can change this text\\n\",\n",
    "    \"audio_file = text_to_speech(input_text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display audio player in Colab\\n\",\n",
    "    \"Audio(audio_file)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4. Patch Wav2Lip for CPU and Librosa 0.9.2 Compatibility\\n\",\n",
    "    \"\\n\",\n",
    "    \"The original Wav2Lip code requires some adjustments to run on CPU and with `librosa==0.9.2`.\\n\",\n",
    "    \"This cell will overwrite the `Wav2Lip/audio.py` file with a version compatible with our setup.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%%writefile /content/Wav2Lip/audio.py\\n\",\n",
    "    \"import librosa\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from scipy.io import wavfile\\n\",\n",
    "    \"import scipy.signal as sps\\n\",\n",
    "    \"\\n\",\n",
    "    \"hparams = {\\n\",\n",
    "    \"    'sample_rate': 16000,\\n\",\n",
    "    \"    'preemphasis': 0.97,\\n\",\n",
    "    \"    'n_fft': 800,\\n\",\n",
    "    \"    'hop_length': 200,\\n\",\n",
    "    \"    'win_length': 800,\\n\",\n",
    "    \"    'num_mels': 80,\\n\",\n",
    "    \"    'fmin': 55,\\n\",\n",
    "    \"    'fmax': 7600,\\n\",\n",
    "    \"    'ref_db': 20,\\n\",\n",
    "    \"    'min_level_db': -100,\\n\",\n",
    "    \"    'rescale': True,\\n\",\n",
    "    \"    'rescaling_max': 0.999,\\n\",\n",
    "    \"    # Mel filters are scaled to be energy-preserving\\n\",\n",
    "    \"    'mel_weight_normalize': True, # Use librosa's Slaney an Tromp normalization\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_wav(path, sr):\\n\",\n",
    "    \"    return librosa.load(path, sr=sr)[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"def save_wav(wav, path, sr):\\n\",\n",
    "    \"    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\\n\",\n",
    "    \"    #proposed by @dsmiller\\n\",\n",
    "    \"    wavfile.write(path, sr, wav.astype(np.int16))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def preemphasis(wav, k, preemphasize=True):\\n\",\n",
    "    \"    if preemphasize:\\n\",\n",
    "    \"        return sps.lfilter([1, -k], [1], wav)\\n\",\n",
    "    \"    return wav\\n\",\n",
    "    \"\\n\",\n",
    "    \"def inv_preemphasis(wav, k, inv_preemphasize=True):\\n\",\n",
    "    \"    if inv_preemphasize:\\n\",\n",
    "    \"        return sps.lfilter([1], [1, -k], wav)\\n\",\n",
    "    \"    return wav\\n\",\n",
    "    \"\\n\",\n",
    "    \"def melspectrogram(wav):\\n\",\n",
    "    \"    D = _stft(preemphasis(wav, hparams['preemphasis']))\\n\",\n",
    "    \"    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams['ref_db']\\n\",\n",
    "    \"    if hparams['rescale']:\\n\",\n",
    "    \"        S = _normalize(S)\\n\",\n",
    "    \"    return S\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _stft(y):\\n\",\n",
    "    \"    return librosa.stft(y=y, n_fft=hparams['n_fft'], hop_length=hparams['hop_length'], win_length=hparams['win_length'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _linear_to_mel(spectrogram):\\n\",\n",
    "    \"    _mel_basis = _build_mel_basis()\\n\",\n",
    "    \"    return np.dot(_mel_basis, spectrogram)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _build_mel_basis():\\n\",\n",
    "    \"    # Use htk=True for Slaney-style MEL weights\\n\",\n",
    "    \"    return librosa.filters.mel(sr=hparams['sample_rate'], n_fft=hparams['n_fft'], n_mels=hparams['num_mels'],\\n\",\n",
    "    \"                               fmin=hparams['fmin'], fmax=hparams['fmax'], htk=True,\\n\",\n",
    "    \"                               norm='slaney' if hparams['mel_weight_normalize'] else None)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _amp_to_db(x):\\n\",\n",
    "    \"    min_level = np.exp(hparams['min_level_db'] / 20 * np.log(10))\\n\",\n",
    "    \"    return 20 * np.log10(np.maximum(min_level, x))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _db_to_amp(x):\\n\",\n",
    "    \"    return np.power(10.0, (x * 0.05))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _normalize(S):\\n\",\n",
    "    \"    return np.clip((S - hparams['min_level_db']) / -hparams['min_level_db'], 0, 1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def _denormalize(S):\\n\",\n",
    "    \"    return (np.clip(S, 0, 1) * -hparams['min_level_db']) + hparams['min_level_db']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Patched Wav2Lip/audio.py created successfully.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Further patch: Ensure face_detect command uses python and handles paths correctly for subprocess\\n\",\n",
    "    \"# The inference script might also need small tweaks for CPU, handled in the next step's command line call.\\n\",\n",
    "    \"def patch_inference_file():\\n\",\n",
    "    \"    inference_path = '/content/Wav2Lip/inference.py'\\n\",\n",
    "    \"    with open(inference_path, 'r') as f:\\n\",\n",
    "    \"        content = f.read()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ensure device is CPU\\n\",\n",
    "    \"    content = content.replace('device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")',\\n\",\n",
    "    \"                              'device = torch.device(\\\"cpu\\\")')\\n\",\n",
    "    \"    content = content.replace('model = model.to(device)', 'model = model.to(torch.device(\\\"cpu\\\"))')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make sure subprocess calls for face detection use full python path if necessary and quote paths\\n\",\n",
    "    \"    # This is more of a safeguard, the main call will be python inference.py ...\\n\",\n",
    "    \"    content = content.replace(\\\"subprocess.call([args.face_detection_script,\\\",\\n\",\n",
    "    \"                              \\\"subprocess.call(['python', args.face_detection_script,\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    with open(inference_path, 'w') as f:\\n\",\n",
    "    \"        f.write(content)\\n\",\n",
    "    \"    print(f\\\"Patched {inference_path} for CPU usage and subprocess calls.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"patch_inference_file()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5. Run Wav2Lip Inference\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now, we'll run the Wav2Lip inference script. \\n\",\n",
    "    \"This will take the static image (`/content/avatar.jpg`) and the generated audio (`/content/generated_tts.wav`) to produce a lip-synced video.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Important Notes:**\\n\",\n",
    "    \"- Ensure the `checkpoint_path` points to the downloaded model.\\n\",\n",
    "    \"- `face` is the path to your input image.\\n\",\n",
    "    \"- `audio` is the path to your input audio.\\n\",\n",
    "    \"- `outfile` is where the result video will be saved.\\n\",\n",
    "    \"- We add `--device cpu` to explicitly use the CPU. While we patched `inference.py`, this is an additional safeguard.\\n\",\n",
    "    \"- `--pads` and `--face_det_batch_size` are adjusted for potentially slower CPU processing.\\n\",\n",
    "    \"- If you see errors related to `ffmpeg`, it might not be installed or found in Colab's default environment. The script usually handles this, but it's a common point of failure if misconfigured.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define file paths\\n\",\n",
    "    \"face_image_path = \\\"/content/avatar.jpg\\\"\\n\",\n",
    "    \"audio_input_path = \\\"/content/generated_tts.wav\\\"\\n\",\n",
    "    \"output_video_path = \\\"/content/Wav2Lip/results/result.mp4\\\" # Save it within the Wav2Lip results folder first\\n\",\n",
    "    \"final_output_path = \\\"/content/result.mp4\\\" # Final path for easy access\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Wav2Lip Inference Command\\n\",\n",
    "    \"# Using !python instead of !cd Wav2Lip && python ... to simplify path management for input/output\\n\",\n",
    "    \"wav2lip_command = (\\n\",\n",
    "    \"    f\\\"python /content/Wav2Lip/inference.py \\\"\\n\",\n",
    "    \"    f\\\"--checkpoint_path /content/Wav2Lip/checkpoints/wav2lip_gan.pth \\\"\\n\",\n",
    "    \"    f\\\"--face {face_image_path} \\\"\\n\",\n",
    "    \"    f\\\"--audio {audio_input_path} \\\"\\n\",\n",
    "    \"    f\\\"--outfile {output_video_path} \\\"\\n\",\n",
    "    \"    f\\\"--device cpu \\\" # Explicitly set device to CPU\\n\",\n",
    "    \"    f\\\"--pads 0 10 0 0 \\\" # Adjust padding as needed\\n\",\n",
    "    \"    f\\\"--face_det_batch_size 4 \\\" # Lower batch size for CPU\\n\",\n",
    "    \"    f\\\"--wav2lip_batch_size 32\\\" # Adjust based on CPU capability\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Running Wav2Lip command: {wav2lip_command}\\\")\\n\",\n",
    "    \"# subprocess.run(wav2lip_command, shell=True, check=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    process = subprocess.run(wav2lip_command, shell=True, check=True, capture_output=True, text=True)\\n\",\n",
    "    \"    print(\\\"Wav2Lip process completed successfully.\\\")\\n\",\n",
    "    \"    print(\\\"STDOUT:\\\")\\n\",\n",
    "    \"    print(process.stdout)\\n\",\n",
    "    \"    if process.stderr:\\n\",\n",
    "    \"        print(\\\"STDERR: (should be empty on success)\\\")\\n\",\n",
    "    \"        print(process.stderr)\\n\",\n",
    "    \"except subprocess.CalledProcessError as e:\\n\",\n",
    "    \"    print(f\\\"Wav2Lip process failed with exit status {e.returncode}.\\\")\\n\",\n",
    "    \"    print(\\\"STDOUT:\\\")\\n\",\n",
    "    \"    print(e.stdout)\\n\",\n",
    "    \"    print(\\\"STDERR:\\\")\\n\",\n",
    "    \"    print(e.stderr)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Move the result to /content for easier access if needed, and to match the plan's output path\\n\",\n",
    "    \"if os.path.exists(output_video_path):\\n\",\n",
    "    \"    os.rename(output_video_path, final_output_path)\\n\",\n",
    "    \"    print(f\\\"Output video saved as {final_output_path}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Error: Output video not found at {output_video_path}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 6. Display the Output Video\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell will display the generated lip-synced video directly in the notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from IPython.display import HTML\\n\",\n",
    "    \"from base64 import b64encode\\n\",\n",
    "    \"\\n\",\n",
    "    \"video_path = \\\"/content/result.mp4\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(video_path):\\n\",\n",
    "    \"    mp4 = open(video_path,'rb').read()\\n\",\n",
    "    \"    data_url = \\\"data:video/mp4;base64,\\\" + b64encode(mp4).decode()\\n\",\n",
    "    \"    display(HTML(f'''\\n\",\n",
    "    \"    <video width=400 controls>\\n\",\n",
    "    \"          <source src=\\\"{data_url}\\\" type=\\\"video/mp4\\\">\\n\",\n",
    "    \"    </video>'''))\\n\",\n",
    "    \"    print(f\\\"Displaying video from {video_path}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Video file not found at {video_path}. Please ensure the Wav2Lip script ran successfully.\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
